\documentclass[10pt,twocolumn]{article}
\usepackage[utf8]{inputenc}
\usepackage[margin=0.2in]{geometry}
\usepackage[compact]{titlesec}
\titlespacing{\section}{0pt}{2ex}{1ex}
\titlespacing{\subsection}{0pt}{1.5ex}{1ex}
\titlespacing{\subsubsection}{0pt}{0.5ex}{0ex}
\usepackage{amsmath,amsthm,amssymb,MnSymbol}
\setlength{\parindent}{0in}
\renewenvironment{proof}{{\bfseries Proof}}{\\}
\newcommand{\newlinetab}[0]{$\text{ }\hspace{3mm}$}

%--------------------------------------------------------
% Statistical Machine Learning 702 Cheatsheet for Midterm
%--------------------------------------------------------

\begin{document}
    %The \textbf{mean} or \textbf{expected value} of $g(X)$ is
    %\begin{equation}
    %\mathbb{E}(g(X)) = \int g(x)dF(x) = \int g(x)dP(x)
    %\end{equation}
    %Related properties and definitions:
\section*{SML 10/36-702 Cheatsheet}

%\subsection*{Expected Value Related}
    \textbf{Some Expectation-related Defs and Properties}
    \begin{flalign}
        (a)& \hspace{2mm} \mathbb{E}(g(X)) = \int g(x)dP(x), \hspace{2mm} \mu = \mathbb{E}(X)\\
        (b)& \hspace{2mm} \mathbb{E}\left(\sum_{i} c_{i}g_{i}(X_{i})\right) = \sum_{i} c_{i} \mathbb{E}(g_{i}(X_{i})) \\
        (c)& \hspace{2mm} \mathbb{E}\left(\prod_{i} X_{i} \right) = \prod_{i} \mathbb{E}(X_{i}), \hspace{4mm} X_{1}, \ldots, X_{n} \text{ indep't} \\
        (d)& \hspace{2mm} Var(X) = \sigma^{2} = \mathbb{E}\left((X-\mu)^{2}\right) = \mathbb{E}\left(X^{2}\right) - \mu^{2}\\
        (e)& \hspace{2mm} Var \left (\sum_{i} a_{i}X_{i} \right ) = \sum_{i} a_{i}^{2} Var(X_{i}), \hspace{4mm} X_{1}, \ldots, X_{n} \text{ indep't} \\
        (f)& \hspace{2mm} Cov(X,Y) = \mathbb{E}((X-\mu_{X})(Y-\mu_{Y})) \hspace{2mm} \text{is the \textbf{covariance}} \\
        (g)& \hspace{2mm} Cov(X,Y) = \mathbb{E}(XY) - \mu_{x}\mu_{Y} \\
        (h)& \hspace{2mm} \rho(X,Y) = Cov(X,Y) / \sigma_{x}\sigma_{y}, \hspace{4mm} -1 \leq \rho(X,Y) \leq 1
    \end{flalign}
    The \textbf{conditional expectation} of Y given X is the random variable $g(X) = \mathbb{E}(Y|X)$, where
    \begin{equation}
        \mathbb{E}(Y|X=x) = \int y f(y|x)dy, \hspace{3mm} f(y|x) = f_{X,Y}(x,y) / f_{X}(x)
    \end{equation}
    The \emph{Law of Total/Iterated Expectation} is
    \begin{equation}
    \mathbb{E}(Y) = \mathbb{E}[\mathbb{E}(Y|X)]
    \end{equation}
    The \emph{Law of Total Variance} is
    \begin{equation}
    Var(Y) = Var[\mathbb{E}(Y|X)] + \mathbb{E}[Var(Y|X)]
    \end{equation}

\subsection*{Concentration of Measure}
    \textbf{Thm (Gaussian Tail Inequality):}
    Let $X \sim \mathcal{N}(0,1)$. Then
    \begin{equation}
        \mathbb{P}(|X| > \epsilon) \leq \frac{2}{\epsilon}e^{-\epsilon^{2}/2} \hspace{2mm}\text{ and } \hspace{2mm}
        \mathbb{P}(|\overline{X}_{n}| > \epsilon) \leq \frac{1}{\sqrt{n}\epsilon}e^{-n\epsilon^{2}/2}
    \end{equation}
        
    \textbf{Thm (Markov Inequality):}
    Let X be a non-negative random variable s.t. $\mathbb{E}(X)$ exists. Then $\forall$ $t>0$
    \begin{equation}
        \mathbb{P}(X>t) \leq \frac{\mathbb{E}(X)}{t}
    \end{equation}

    \textbf{Thm (Chebyshev's Inequality):}
    Let $\mu = \mathbb{E}(X)$ and $\sigma^{2} = \text{Var}(X)$. Then
    \begin{equation}
        \mathbb{P}(|X-\mu| \geq t) \leq \frac{\sigma^{2}}{t^{2}} \hspace{2mm} \text{ i.e. } \hspace{2mm}
        \mathbb{P}(|(X-\mu)/\sigma| \geq t) \leq \frac{1}{t^{2}}
    \end{equation}

    \textbf{Lemma:}
    Let $\mathbb{E}(X) = 0$ and $a \leq X \leq b$. Then
    \begin{equation}
        \mathbb{E}(e^{tX}) \leq e^{t^{2}(b-a)^{2}/8}
    \end{equation}

    \textbf{Lemma (Chernoff's Method):}
    Let $X$ be any random variable. Then $\forall t>0$
    \begin{equation}
        \mathbb{P}(X>\epsilon) \leq \mathbb{P}(e^{tX}>e^{t\epsilon}) \leq e^{-t\epsilon} \mathbb{E}(e^{tX})
    \end{equation}

    \textbf{Thm (Hoeffding's Inequality):}
    $X_{1},\ldots,X_{n}$ iid, $a \leq X_{i} \leq b$. Then $\forall \epsilon >0$
    \begin{equation}
        \mathbb{P} \left(\left| \overline{X} - \mathbb{E}\left(\overline{X}\right) \right| \geq \epsilon \right) \leq 2e^{-2n\epsilon^{2}/(b-a)^{2}}
    \end{equation}

    \textbf{Thm (Bernstein's Inequality):}
    $X_{1},\ldots,X_{n}$ iid, $-c \leq X_{i} \leq c$, $\sigma^2 = \frac{1}{n}\sum_i \text{Var}(X_i)$. Then $\forall \epsilon >0$
    \begin{equation}
        \mathbb{P} \left(\left| \overline{X} - \mathbb{E}\left(\overline{X}\right) \right| \geq \epsilon \right) \leq 2e^{-n\epsilon^2/(2\sigma^2 + 2c\epsilon/3)}
    \end{equation}

    \textbf{Thm (McDiarmid):} $X_{1},\ldots,X_{n}$ indep't. If\\
    $\sup_{x_{1},\ldots,x_{n},x'_{i}} \left| g(x_{1},\ldots,x_{n}) - g_{i}^{*}(x_{1},\ldots,x_{n}) \right|$ $\leq c_{i}$ holds $\forall i$ $\implies$
    \begin{equation}
        \mathbb{P} \left(\left| g(X_{1},\ldots,X_{n})-\mathbb{E}(g(X_{1},\ldots,X_{n})) \right| \geq \epsilon \right) \leq 2e^{-2\epsilon^{2}/\sum_{i}c_{i}^{2}}
    \end{equation}
    where $g_{i}^{*} = g$ with $x_{i}$ replaced by $x'_{i}$.

\subsection*{Uniform Bounds}
    The \textbf{union bound} is 
    \begin{equation}
        \mathbb{P} \left( \sup_{f \in \mathcal{F}} \left| \hat{R} - R  \right| > \epsilon \right) 
            \leq |\mathcal{F}| e^{\text{something}}
    \end{equation}
    \textbf{VC Theory:}
    $F$ a finite set, $|F| = n$, and $G \subset F$. $\mathcal{A}$ is a class of sets.
    $\mathcal{A}$ \textbf{picks out} $G$ if $\exists A \in \mathcal{A}$ s.t. $A \cap F = G$.
    Let $S(\mathcal{A},F)$ $=$ $\#\{G \subset F \text{ picked out by } \mathcal{A}\}$ $\leq 2^{n}$.
    $F$ is \textbf{shattered} by $\mathcal{A}$ if $S(\mathcal{A},F) = 2^{n}$ (ie if $\mathcal{A}$ picks out all $G \subset F$).
    Let $\mathcal{F}_{n}$ be all finite sets with $n$ elements.
    The \textbf{shatter coefficient} $s(\mathcal{A},n) = \sup_{F \in \mathcal{F}_{n}} S(\mathcal{A},F) \leq 2^{n}$.
    The \textbf{VC dimension} $d(\mathcal{A}) =$ the largest $n$ s.t. $s(\mathcal{A},n) = 2^{n}$.\\
    \textbf{Thm (Vapnik and Chervonenkis):} $\forall t>\sqrt{2/n}$, 
    \begin{equation}
        \mathbb{P} \left( \sup_{A \in \mathcal{A}} |P_{n}(A) - P(A)| > t \right) \leq 4 s(\mathcal{A},2n)e^{-nt^{2}/8}
    \end{equation}
    \textbf{Rademacher Complexity:} Assume $\mathcal{F}$ a class of functions s.t. $0 \leq f(z) \leq 1, \forall f \in \mathcal{F}$.
    $\sigma_1,\ldots,\sigma_n$ are \emph{Rademacher random variables} if they are iid and $\mathbb{P}(\sigma_i=1) = \mathbb{P}(\sigma_i=-1) = 1/2$.
    Then the \emph{Rademacher complexity} of $\mathcal{F}$ is:
    \begin{equation}
        \text{Rad}_n(\mathcal{F}) = \mathbb{E} \left( \sup_{f \in \mathcal{F}} \left| \frac{1}{n} \sum_i \sigma_i f(Z_i)  \right|  \right)
    \end{equation}
    where the expectation is over $\sigma$ only.\\
    \textbf{Thm:} With probability $\geq 1-\delta$
    \begin{equation}
        \sup_{f \in \mathcal{F}} \left| P_n(f) - P(f)  \right|
            \leq 2\text{Rad}_n(\mathcal{F}) + \sqrt{\frac{1}{2n} \text{log} \left( \frac{2}{\delta} \right)}
    \end{equation}

\subsection*{Function Spaces}
    A \textbf{vector space} is closed under addition and scalar multiplication. 
    A vector space of functions is a \textbf{function space}. 
    A normed space is \textbf{complete} if every \emph{Cauchy sequence} converges to a limit. 
    A complete, normed space is a \textbf{Banach space}. 
    A complete, inner product space is a \textbf{Hilbert space}.% (also a normed space).
    A Hilbert space is \textbf{separable} if there exists a countable orthonormal basis.

    Let $\mathcal{F}$ be a set of functions $f:[a,b] \rightarrow \mathbb{R}$.  
    The \textbf{$L_p$ norm} on $\mathcal{F}$ is $\| f \|_p = \left( \int_a^b |f(x)|^p dx  \right)^{1/p}$\hspace{-2mm}. 
    The space $L_p(a,b)$ $=$ $\left\{  f : \| f \|_p < \infty \right\} \subset \mathcal{F}$.
    $L_2(a,b)$ with inner product $\int_a^b f(x)g(x) dx$ is a separable Hilbert space.

    A fn $f$ is \textbf{weakly diff'able} if $\exists f'$ s.t. $\int_x^y f'(s) ds = f(y) - f(x)$.
    Let $D^jf$ denote the $j^{\text{th}}$ weak derivative of $f$.
    The \textbf{Sobolev space} of order $m$ is $W_{m,p}$ $=$ $\left\{ f \in L_p(0,1) : \|D^m f\| \in L_p(0,1) \right\}$.

    \textbf{Lipschitz space:} $\text{Lip}(c)$ $=$ $\left\{ f : \sup_x |f(x)| \leq c, \sup_{x,y} \frac{|f(y)-f(x)|}{|y-x|} \right\}$
    \textbf{Holder space:} $H_{\alpha}(c)$ $=$ $\left\{ f : \frac{\partial^{\alpha} f}{\partial x^{\alpha}} \in \text{Lip}(c) \right\}$ 
        if $\alpha \in \mathbb{Z}$ and\\
    $H_{\alpha+\beta}(c) = \left\{ f : f \in H_{\alpha}(c), \frac{\partial^{\alpha} f}{\partial x^{\alpha}} \in H_{\beta}(c) \right\}$ 
        if $\alpha \in \mathbb{Z}$, $0 < \beta < 1$.

\subsection*{Density estimation}
    The goal is to estimate $p(x)$ with estimator $\hat{p}(x) = \hat{p}_h(x)$. \\
    The \textbf{$L_2$ loss} $= \int \left( \hat{p}(x) - p(x) \right)^2 dx$.
    The \textbf{risk} $R(p,\hat{p})$ $=$ $\mathbb{E}(L(p,\hat{p}))$.

    The \textbf{histogram density estimator} is 
        $\hat{p}_h(x)$ $=$ $\sum_{j=1}^N \frac{\hat{\theta}_j}{h^d} \mathbb{I}(x \in B_j)$
        where $\hat{\theta}_j = \frac{1}{n} \sum_{i=1}^n \mathbb{I}(X_i \in B_j)$.\\
    The \textbf{kernel density estimator} is 
    $\hat{p}_h(x)$ $=$ $\frac{1}{n} \sum_{i=1}^n \frac{1}{h^d} K \left( \frac{\| x-X_i \|}{h}  \right).$
    E.g. \emph{Gaussian kernel} $K(x)$ $=$ $\frac{1}{\sqrt{2\pi}} e^{-x^2/2}$.\\
    %Let $p_h(x) = \mathbb{E}(\hat{p}_h(x))$.\\
    \textbf{Lemma (kde bias bound):} for some $c$, 
    \begin{equation}
        \sup_{p \in H_{\beta}(L)} |\mathbb{E}\left(\hat{p}_h(x)\right) - p(x)| \leq ch^{\beta}
    \end{equation}
    \textbf{Lemma (kde variance bound):} for some $c>0$, 
    \begin{equation}
        \sup_{p \in H_{\beta}(L)} \text{Var}(\hat{p}_h(x)) \leq \frac{c}{nh^d}
    \end{equation}
    holder class, bias, variance, risk, concentration

\subsection*{Nonparametric Classification}
    classifiers: plug-in, histogram, decision trees

\subsection*{Nonparametric Regression}
    regressors: kernel estimators, local polynomials, RKHS estimators

\subsection*{Minimax}
    Minimax risk definition\\
    Upper bound risk\\
    Lower bound risk (Le Cam and Fano)

\end{document}
