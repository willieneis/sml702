\documentclass[10pt,twocolumn]{article}
\usepackage[utf8]{inputenc}
\usepackage[margin=0.2in]{geometry}
\usepackage[compact]{titlesec}
\titlespacing{\section}{0pt}{2ex}{1ex}
\titlespacing{\subsection}{0pt}{1.5ex}{1ex}
\titlespacing{\subsubsection}{0pt}{0.5ex}{0ex}
\usepackage{amsmath,amsthm,amssymb,MnSymbol,paralist}
\setlength{\parindent}{0in}
\renewenvironment{proof}{{\bfseries Proof}}{\\}
\newcommand{\newlinetab}[0]{$\text{ }\hspace{3mm}$}

%--------------------------------------------------------
% Statistical Machine Learning 702 Cheatsheet for Midterm
%--------------------------------------------------------

\begin{document}

\section*{SML 10/36-702 Cheatsheet}

\textbf{Some Expectation-related Defs and Properties}
    \begin{flalign}
        (a)& \hspace{2mm} \mathbb{E}(g(X)) = \int g(x)dP(x), \hspace{2mm} \mu = \mathbb{E}(X)\\
        (b)& \hspace{2mm} \mathbb{E}\left(\sum_{i} c_{i}g_{i}(X_{i})\right) = \sum_{i} c_{i} \mathbb{E}(g_{i}(X_{i})) \\
        (c)& \hspace{2mm} \mathbb{E}\left(\prod_{i} X_{i} \right) = \prod_{i} \mathbb{E}(X_{i}), \hspace{4mm} X_{1}, \ldots, X_{n} \text{ indep't} \\
        (d)& \hspace{2mm} Var(X) = \sigma^{2} = \mathbb{E}\left((X-\mu)^{2}\right) = \mathbb{E}\left(X^{2}\right) - \mu^{2}\\
        (e)& \hspace{2mm} Var \left (\sum_{i} a_{i}X_{i} \right ) = \sum_{i} a_{i}^{2} Var(X_{i}), \hspace{4mm} X_{1}, \ldots, X_{n} \text{ indep't} \\
        (f)& \hspace{2mm} Cov(X,Y) = \mathbb{E}((X-\mu_{X})(Y-\mu_{Y})) \hspace{2mm} \text{is the \textbf{covariance}} \\
        (g)& \hspace{2mm} Cov(X,Y) = \mathbb{E}(XY) - \mu_{x}\mu_{Y} \\
        (h)& \hspace{2mm} \rho(X,Y) = Cov(X,Y) / \sigma_{x}\sigma_{y}, \hspace{4mm} -1 \leq \rho(X,Y) \leq 1
    \end{flalign}
    The \textbf{conditional expectation} of Y given X is the random variable $g(X) = \mathbb{E}(Y|X)$, where
    \begin{equation}
        \mathbb{E}(Y|X=x) = \int y f(y|x)dy, \hspace{3mm} f(y|x) = f_{X,Y}(x,y) / f_{X}(x)
    \end{equation}
    The \emph{Law of Total/Iterated Expectation} is
    \begin{equation}
    \mathbb{E}(Y) = \mathbb{E}[\mathbb{E}(Y|X)]
    \end{equation}
    The \emph{Law of Total Variance} is
    \begin{equation}
    Var(Y) = Var[\mathbb{E}(Y|X)] + \mathbb{E}[Var(Y|X)]
    \end{equation}

\subsection*{Concentration of Measure}
    \textbf{Thm (Gaussian Tail Inequality):}
    Let $X \sim \mathcal{N}(0,1)$. Then
    \begin{equation}
        \mathbb{P}(|X| > \epsilon) \leq \frac{2}{\epsilon}e^{-\epsilon^{2}/2} \hspace{2mm}\text{ and } \hspace{2mm}
        \mathbb{P}(|\overline{X}_{n}| > \epsilon) \leq \frac{1}{\sqrt{n}\epsilon}e^{-n\epsilon^{2}/2}
    \end{equation}
        
    \textbf{Thm (Markov Inequality):}
    Let X be a non-negative random variable s.t. $\mathbb{E}(X)$ exists. Then $\forall$ $t>0$
    \begin{equation}
        \mathbb{P}(X>t) \leq \frac{\mathbb{E}(X)}{t}
    \end{equation}

    \textbf{Thm (Chebyshev's Inequality):}
    Let $\mu = \mathbb{E}(X)$ and $\sigma^{2} = \text{Var}(X)$. Then
    \begin{equation}
        \mathbb{P}(|X-\mu| \geq t) \leq \frac{\sigma^{2}}{t^{2}} \hspace{2mm} \text{ i.e. } \hspace{2mm}
        \mathbb{P}(|(X-\mu)/\sigma| \geq t) \leq \frac{1}{t^{2}}
    \end{equation}

    \textbf{Lemma:}
    Let $\mathbb{E}(X) = 0$ and $a \leq X \leq b$. Then
    \begin{equation}
        \mathbb{E}(e^{tX}) \leq e^{t^{2}(b-a)^{2}/8}
    \end{equation}

    \textbf{Lemma (Chernoff's Method):}
    Let $X$ be any random variable. Then $\forall t>0$
    \begin{equation}
        \mathbb{P}(X>\epsilon) \leq \mathbb{P}(e^{tX}>e^{t\epsilon}) \leq e^{-t\epsilon} \mathbb{E}(e^{tX})
    \end{equation}

    \textbf{Thm (Hoeffding's Inequality):}
    $X_{1},\ldots,X_{n}$ iid, $a \leq X_{i} \leq b$. Then $\forall \epsilon >0$
    \begin{equation}
        \mathbb{P} \left(\left| \overline{X} - \mathbb{E}\left(\overline{X}\right) \right| \geq \epsilon \right) \leq 2e^{-2n\epsilon^{2}/(b-a)^{2}}
    \end{equation}

    \textbf{Thm (Bernstein's Inequality):}
    $X_{1},\ldots,X_{n}$ iid, $-c \leq X_{i} \leq c$, $\sigma^2 = \frac{1}{n}\sum_i \text{Var}(X_i)$. Then $\forall \epsilon >0$
    \begin{equation}
        \mathbb{P} \left(\left| \overline{X} - \mathbb{E}\left(\overline{X}\right) \right| \geq \epsilon \right) \leq 2e^{-n\epsilon^2/(2\sigma^2 + 2c\epsilon/3)}
    \end{equation}

    \textbf{Thm (McDiarmid):} $X_{1},\ldots,X_{n}$ indep't. If\\
    $\sup_{x_{1},\ldots,x_{n},x'_{i}} \left| g(x_{1},\ldots,x_{n}) - g_{i}^{*}(x_{1},\ldots,x_{n}) \right|$ $\leq c_{i}$ holds $\forall i$ $\implies$
    \begin{equation}
        \mathbb{P} \left(\left| g(X_{1},\ldots,X_{n})-\mathbb{E}(g(X_{1},\ldots,X_{n})) \right| \geq \epsilon \right) \leq 2e^{-2\epsilon^{2}/\sum_{i}c_{i}^{2}}
    \end{equation}
    where $g_{i}^{*} = g$ with $x_{i}$ replaced by $x'_{i}$.

\subsection*{Uniform Bounds}
    For a finite set of binary functions $\mathcal{F} = \{ f_1, \ldots f_N \}$, $\forall \epsilon>0$,
    \begin{equation}
        \mathbb{P}\left( \max_{f \in \mathcal{F}} |P_n(f) - P(f)| > \epsilon \right) 
        \leq 2Ne^{-n\epsilon^2/2}
    \end{equation}
    \textbf{VC Theory:}
    $F$ a finite set, $|F| = n$, and $G \subset F$. $\mathcal{A}$ is a class of sets.
    $\mathcal{A}$ \textbf{picks out} $G$ if $\exists A \in \mathcal{A}$ s.t. $A \cap F = G$.
    Let $S(\mathcal{A},F)$ $=$ $\#\{G \subset F \text{ picked out by } \mathcal{A}\}$ $\leq 2^{n}$.
    $F$ is \textbf{shattered} by $\mathcal{A}$ if $S(\mathcal{A},F) = 2^{n}$ (ie if $\mathcal{A}$ picks out all $G \subset F$).
    Let $\mathcal{F}_{n}$ be all finite sets with $n$ elements.
    The \textbf{shatter coefficient} $s(\mathcal{A},n) = \sup_{F \in \mathcal{F}_{n}} S(\mathcal{A},F) \leq 2^{n}$.
    The \textbf{VC dimension} $d(\mathcal{A}) =$ the largest $n$ s.t. $s(\mathcal{A},n) = 2^{n}$.\\
    \textbf{Thm (Vapnik and Chervonenkis):} $\forall t>\sqrt{2/n}$, 
    \begin{equation}
        \mathbb{P} \left( \sup_{A \in \mathcal{A}} |P_{n}(A) - P(A)| > t \right) \leq 4 s(\mathcal{A},2n)e^{-nt^{2}/8}
    \end{equation}
    \textbf{Rademacher Complexity:} Assume $\mathcal{F}$ a class of functions s.t. $0 \leq f(z) \leq 1, \forall f \in \mathcal{F}$.
    $\sigma_1,\ldots,\sigma_n$ are \emph{Rademacher random variables} if they are iid and $\mathbb{P}(\sigma_i=1) = \mathbb{P}(\sigma_i=-1) = 1/2$.
    Then the \emph{Rademacher complexity} of $\mathcal{F}$ is:
    \begin{equation}
        \text{Rad}_n(\mathcal{F}) = \mathbb{E} \left( \sup_{f \in \mathcal{F}} \left| \frac{1}{n} \sum_i \sigma_i f(Z_i)  \right|  \right)
    \end{equation}
    where the expectation is over $\sigma$ only.\\
    \textbf{Thm:} With probability $\geq 1-\delta$
    \begin{equation}
        \sup_{f \in \mathcal{F}} \left| P_n(f) - P(f)  \right|
            \leq 2\text{Rad}_n(\mathcal{F}) + \sqrt{\frac{1}{2n} \text{log} \left( \frac{2}{\delta} \right)}
    \end{equation}

\subsection*{Function Spaces}
    A \textbf{vector space} is closed under addition and scalar multiplication. 
    A vector space of functions is a \textbf{function space}. 
    A normed space is \textbf{complete} if every \emph{Cauchy sequence} converges to a limit. 
    A complete, normed space is a \textbf{Banach space}.\\
    \textbf{Inner product}: a mapping $\langle\cdot,\cdot\rangle: V \times V \rightarrow \mathbb{R} \; \text{s.t.} \; \forall x,y,z \in V, a \in \mathbb{R}:$
    \begin{enumerate}
        \itemsep-.5em
        \item $\langle x,x \rangle \geq 0, \langle x,x \rangle = 0 \Leftrightarrow x = 0$
        \item $\langle x,y+z \rangle  = \langle x,y \rangle + \langle x,z \rangle$
        \item $\langle x, ay \rangle = a\langle x,y \rangle$
        \item $\langle x,y \rangle = \langle y,x \rangle$.
    \end{enumerate}
    \vspace{-3mm}Note that $\|v\| = \sqrt{\langle v,v  \rangle}$.

    A complete, inner product space is a \textbf{Hilbert space}. % (also a normed space).
    A Hilbert space is \textbf{separable} if there exists a countable orthonormal basis.

    \textbf{Thm:} If $V$ a separable Hilbert space with countable orthonormal basis 
        $\{ e_1,e_2,\ldots \}$ $\implies$ $\forall x \in V$, $x = \sum_{j=1}^{\infty}\theta_je_j$, 
        where $\theta_j = \langle x, e_j \rangle$, and also $||x||^2 = \sum_{j=1}^{\infty}\theta_j^2$.

    Let $\mathcal{F}$ be a set of functions $f:[a,b] \rightarrow \mathbb{R}$.
    The \textbf{$L_p$ norm} on $\mathcal{F}$ is $\| f \|_p = \left( \int_a^b |f(x)|^p dx  \right)^{1/p}$\hspace{-2mm}. 
    The space $L_p(a,b)$ $=$ $\left\{  f : \| f \|_p < \infty \right\} \subset \mathcal{F}$.
    $L_2(a,b)$ with inner product $\int_a^b f(x)g(x) dx$ is a separable Hilbert space.

    A fn $f$ is \textbf{weakly diff'able} if $\exists f'$ s.t. $\int_x^y f'(s) ds = f(y) - f(x)$.
    Let $D^jf$ denote the $j^{\text{th}}$ weak derivative of $f$.
    The \textbf{Sobolev space} of order $m$ is $W_{m,p}$ $=$ $\left\{ f \in L_p(0,1) : \|D^m f\| \in L_p(0,1) \right\}$.

    \textbf{Lipschitz space:} $\text{Lip}(c)$ $=$ $\left\{ f : \sup_x |f(x)| \leq c, \sup_{x,y} \frac{|f(y)-f(x)|}{|y-x|} \right\}$
\textbf{Holder space:} for $0<\beta\leq 1$, the \textbf{Holder space} or \textbf{class} is\\
    $H_{\beta}(c) = \left\{ f : \sup_x |f(x)| \leq c, 
        \sup_{x,y} \frac{|f(y)-f(x)|}{|y-x|^{\beta}} \leq c \right\}$\\
    Note if $\alpha \in \mathbb{Z}$, $H_{\alpha}(c)$ $=$ $\left\{ f : \frac{\partial^{\alpha} f}{\partial x^{\alpha}} \in \text{Lip}(c) \right\}$ and\\
    if $\alpha \in \mathbb{Z}$, $0 < \beta < 1$, $H_{\alpha+\beta}(c) 
        = \left\{ f : f \in H_{\alpha}(c), \frac{\partial^{\alpha} f}{\partial x^{\alpha}} \in H_{\beta}(c) \right\}$

\subsection*{Density estimation}
    The goal is to estimate $p(x)$ with estimator $\hat{p}(x) = \hat{p}_h(x)$. \\
    The \textbf{$L_2$ loss} $= \int \left( \hat{p}(x) - p(x) \right)^2 dx$.
    The \textbf{risk} $R(p,\hat{p})$ $=$ $\mathbb{E}(L(p,\hat{p}))$.

    The \textbf{histogram density estimator} is 
        $\hat{p}_h(x)$ $=$ $\sum_{j=1}^N \frac{\hat{\theta}_j}{h^d} \mathbb{I}(x \in B_j)$
        where $\hat{\theta}_j = \frac{1}{n} \sum_{i=1}^n \mathbb{I}(X_i \in B_j)$.\\
    The \textbf{kernel density estimator} is 
    $\hat{p}_h(x)$ $=$ $\frac{1}{n} \sum_{i=1}^n \frac{1}{h^d} K \left( \frac{\| x-X_i \|}{h}  \right).$
    E.g. \emph{Gaussian kernel} $K(x)$ $=$ $\frac{1}{\sqrt{2\pi}} e^{-x^2/2}$.\\
    %Let $p_h(x) = \mathbb{E}(\hat{p}_h(x))$.\\
    \textbf{Lemma (kde bias bound):} for some $c$, 
    \begin{equation}
        \sup_{p \in H_{\beta}(L)} |\mathbb{E}\left(\hat{p}_h(x)\right) - p(x)| \leq ch^{\beta}
    \end{equation}
    \textbf{Lemma (kde variance bound):} for some $c>0$, 
    \begin{equation}
        \sup_{p \in H_{\beta}(L)} \text{Var}(\hat{p}_h(x)) \leq \frac{c}{nh^d}
    \end{equation}
    Note: $a_n \asymp b_n$ means $a_n/b_n$ and $b_n/a_n$ are bounded for large $n$.\\
    \textbf{Thm (kde risk bound):} If $h \asymp n^{-1/(2\beta + d)}$ then
    \begin{equation}
        \sup_{p \in H_b(L)} \mathbb{E} \left( \int (\hat{p}_h(x) - p(x))^2 dx \right) 
        \leq \frac{c}{n^{2\beta/(2\beta+d)}}
    \end{equation}
    When $\beta=2$ and $h \asymp n^{-1/(4+d)}$, the bound has rate $n^{-4/(4+d)}$.\\
    \textbf{Thm (kde concentration bound):} for small enough $\epsilon >0$,
    \begin{equation}
        \mathbb{P}(|\hat{p}_h(x) - \mathbb{E}(\hat{p}_h(x))| > \epsilon) 
            \leq 2 e^{-cnh^d \epsilon^2}
    \end{equation}
    \textbf{Corr 1:} $\forall \epsilon>0$ and some constants $C$ and $c$,
    \begin{equation}
        \sup_{p \in H_{\beta}(L)} \mathbb{P} \left( | \hat{p}_h(x) - p(x) | > \sqrt{ \frac{C\text{log}(2/\epsilon)}{nh^d} } + ch^{\beta} \right)
            < \epsilon
    \end{equation}
    \textbf{Corr 2:} If $h \asymp n^{-1/(2\beta + d)}$ then $\forall \epsilon >0$,
    \begin{equation}
        \sup_{p \in H_{\beta}(L)} \mathbb{P} \left( | \hat{p}_h(x) - p(x) |^2 > \frac{c}{n^{2\beta/(2\beta + d)}} \right)
            < \epsilon
    \end{equation}
    We also proved that $O(n^{-2\beta/(2\beta + d)})$ is minimax optimal rate.

\subsection*{Nonparametric Regression}
    The goal is to estimate $m(x) = \mathbb{E}(Y|X=x)$ with estimator $\hat{m}(x)$.\\
    The \emph{pointwise risk} is $R(\hat{m}(x) - m(x)) = \mathbb{E}((\hat{m}(x) - m(x))^2)$.\\
    The \emph{predictive risk} is $R(\hat{m},m) = \mathbb{E}((Y - \hat{m}(X))^2)$ \\
        \newlinetab $=$ $\sigma^2 + \mathbb{E}\left( \int(m(x) - \hat{m}(x))^2 dP(x) \right)$.

    \textbf{Kernel estimator:} $\hat{m}_h(x) = \frac{\sum_i Y_i K\left(\frac{\|x-X_i\|}{h}\right)}{\sum_i K\left(\frac{\|x-X_i\|}{h}\right)}$ \\
    If $X_i,\ldots,X_n \sim p(x)$ s.t. $0 < c \leq p(x) \leq C$,
    $p$ has bnded cont's 1st derivs, $m$ has bnded cont's 3rd derivs, then\\
    \textbf{(a) Bias:} $\left| \mathbb{E}(\hat{m}_h(x)) - m(x) \right| \leq c_1 h^4$\\
    \textbf{(b) Variance:} $\text{Var}(\hat{m}_h(x)) \leq \frac{c_2}{nh^d}$\\
    \textbf{(c) Risk:} If $h \asymp n^{-1/(4+d)}$, 
        $\mathbb{E}\left( (\hat{m}_h(x) - m(x))^2 \right) \leq \frac{c}{n^{4/(4+d)}}$\\
    %\textbf{Local polynomial estimator:} \\
    %\textbf{RKHS estimator:}

\subsection*{Nonparametric Classification}
    The goal is to analyze nonparametric classifiers $\hat{h}(X)$.\\
    The \emph{classification risk} is $R(h) = \mathbb{P}(Y \neq h(X))$. 
    This is minimized by \emph{Bayes classifier} $h^*(x) = 1$ if $m(x) = \mathbb{E}(Y|X=x) > 1/2$, 0 o.w.\\
    \textbf{Plug in classifier:} Estimate $m(x)$ with nonparametric regression estimate $\hat{m}(x)$, then plug into Bayes classifier.\\
    \textbf{Histogram classifier:} $\mathcal{H}_m = \{$uniform partitions into $m^d$ bins$\}$. Can bound $R(\hat{h}) - R(h^*)$ with VC Thm, 
        using $\text{VC}(\mathcal{H}_m) = m^d$\\
    \textbf{Decision tree classifier:} $\mathcal{T}_k = \{$decision trees with $k$ leaves$\}$.\\
        Can bound $R(\hat{t}) - R(t^*)$ with VC Thm, using $\text{VC}(\mathcal{T}_k) = k(d+1)$.

\subsection*{Minimax}
    The \textbf{minimax risk} for a set of distributions $\mathcal{P}$ and metric $d$ is
    \begin{equation}
       R_n(\mathcal{P}) = \inf_{\hat{\theta}}  \sup_{P \in \mathcal{P}} \mathbb{E}_P [ d(\hat{\theta}, \theta(P))  ]
    \end{equation}
    The \textbf{sample complexity} $n(\epsilon,\mathcal{P}) = \text{min}\{n : R_n(\mathcal{P}) \leq \epsilon\}$.\\
    The \textbf{KL Divergence} between distributions $P$ and $Q$ is $KL(P,Q) = \int p \text{ log}(p/q)$.

\subsubsection*{Minimax Lower Bounds}
        Components:
    \begin{enumerate}
        \itemsep-.5em
        \item Class of models $\mathcal{F} \subseteq \mathcal{S}$, where $\mathcal{F}$ contains the ``true'' model.
        \item Observation model $\mathcal{P}_f$, where $f \in \mathcal{F}; \mathcal{P}_f$ is the distribution of the data under model $f$.
        \item Performance metric $d(\ldots) \geq 0$.
    \end{enumerate}
    Then, the maximal risk is given by 
    \begin{equation}
        \underset{f\in \mathcal{F}}{\sup} \; \mathbb{E}_f \left[d(\hat{f}_n,f)\right] = \underset{f\in\mathcal{F}}{\sup} \int d(\hat{f}_n(Z),f)d\mathcal{P}_f(Z).
    \end{equation}
    Goal: $\mathcal{R}^*_n := \underset{\hat{f}_n}{\inf}\,\underset{f\in\mathcal{F}}{\sup} \,\mathbb{E} \left[d(\hat{f}_n,f)\right] \geq cs_n$.\\
    General reduction scheme: $\underset{\hat{f}_n}{\inf} \; \underset{f\in\mathcal{F}}{\sup} \; \mathcal{P}_f(d(\hat{f}_n,f) \geq s_n) \geq c > 0$.
    \begin{enumerate}
        \item Replace $\mathcal{F}$ with smaller finite class $\lbrace f_0,\ldots,f_M \rbrace \subseteq \mathcal{F}$. Note: $\underset{\hat{f}_n}{\inf} \; \underset{f\in\mathcal{F}}{\sup} \; \mathcal{P}_f(d(\hat{f}_n,f) \geq s_n) \geq \underset{\hat{f}_n}{\inf} \; \underset{f \in \lbrace f_0,\ldots,f_M \rbrace}{\sup} \; \mathcal{P}_f(d(\hat{f}_n,f) \geq s_n)$.
        \item Reduce to a hypothesis test: $\underset{\hat{f}_n}{\inf} \; \underset{f \in \mathcal{F}}{\sup} \; \mathcal{P}_f(d(\hat{f}_n,f) \geq s_n) \geq \underset{\hat{f}_n}{\inf} \; \underset{f \in \lbrace f_0,\ldots,f_M \rbrace}{\sup} \; \mathcal{P}_{f_j}(\hat{h}_n(Z) \neq j)$, where $\hat{h}_n : \mathcal{Z} \rightarrow \lbrace 0,\ldots,M \rbrace$.
    \end{enumerate}
    \textbf{Lemma 1} Suppose $d(\cdot,\cdot)$ is a semi-distance (i.e., it satisfies 
    \begin{inparaenum}
    \item $d(f,g) = d(g,f) \geq 0$,
    \item $d(f,f) = 0$,
    \item $d(f,g) \geq d(h,f) + d(h,g)$
    \end{inparaenum}
    ). Suppose $f_0,\ldots,f_M$ is s.t. $d(f_j,f_k) \geq 2s_n \, \forall j \neq k$ and $\Psi^* := \arg \underset{j}{\min} \; d(\hat{f}_n,f,f_j)$. Then, $\Psi^*(\hat{f}_n) \neq j \Rightarrow d(\hat{f}_n,f_j) \geq s_n$.\\
    \textbf{Le Cam's Lemma:} Let $\mathcal{F}$ be a class of models, $Z^n$ iid data from $\mathcal{P}_f$, where $f\in\mathcal{F}$ is the true model. If $d$ is a semi-distance and $f_0,f_1 \in \mathcal{F}$ s.t. $d(f_0,f_1) \geq 2s_n$, then 
    \begin{equation}
        \underset{\hat{f}_n}{\inf} \; \underset{f\in\mathcal{F}} {\sup} \; \mathcal{P}_f(d(\hat{f}_n,f) \geq s_n) \geq \frac{1}{4} \exp\lbrace-K(P^n_{f_1} || P^n_{f_0})\rbrace.
    \end{equation}
    \textbf{Corollary} $\underset{\hat{f}_n}{\inf} \; \underset {f\in\mathcal{F}}{\sup} \; \mathbb{E} \lbrack d(\hat{f}_n,f)\rbrack \geq cs_n$.\\
    \textbf{Fano's Lemma:} If $d$ is a semi-distance, $\lbrace f_0,\ldots,f_M \rbrace$ s.t. $d(f_j,f_k) \geq 2s_n \, \forall j \neq k \; (M \geq 2)$, and $\frac{1}{M}\sum_{j=1}^{M} K(P_j||P_0) \leq \alpha \log M \; (0 < \alpha < 1/8)$, then
    \begin{equation}
        \underset{\hat{f}_n}{\inf} \; \underset{f\in\mathcal{F}}{\sup} \; P_f(d(\hat{f}_n,f) \geq s_n) \geq \beta = \frac{\sqrt{M}}{1+\sqrt{M}}\left(1 - 2\alpha - 2\sqrt{\frac{\alpha}{\log M}}\right) > 0
    \end{equation}






\end{document}
